{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the FrozenLake environment\n",
    "env = gym.make('FrozenLake-v1', desc=generate_random_map(size=4), is_slippery=True, render_mode='human')\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Initialize value policy\n",
    "num_states = env.observation_space.n\n",
    "VP = np.zeros(num_states)\n",
    "\n",
    "# Epsilon-greedy policy definition\n",
    "def epsilon_greedy_policy(state, VP, epsilon=0.5):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Choose random action (explore)\n",
    "    else:\n",
    "        return np.argmax(VP[state])  # Choose the best action based on value (exploit)\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "\n",
    "# To track value updates over time\n",
    "values_over_time = []\n",
    "\n",
    "# Run multiple episodes\n",
    "for episode in range(5000):\n",
    "    state, info = env.reset()  # Reset environment at the start of each episode\n",
    "    done = False\n",
    "\n",
    "    while not done:  # Continue until the episode is done\n",
    "        action = epsilon_greedy_policy(state, VP)  # Choose action using epsilon-greedy policy\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)  # Take action and observe result\n",
    "\n",
    "        done = terminated or truncated  # Check if episode has ended\n",
    "\n",
    "        # If it's a terminal state, update based on immediate reward\n",
    "        if done:\n",
    "            VP[state] = reward  # Update terminal state with the reward (since no future state exists)\n",
    "        else:\n",
    "            # Apply the TD(0) update rule for non-terminal states\n",
    "            VP[state] = VP[state] + alpha * (reward + gamma * VP[next_state] - VP[state])\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        print(f\"State: {state}, Next State: {next_state}, Reward: {reward}, VP[{state}]: {VP[state]}\")\n",
    "\n",
    "    # Track value updates after each episode\n",
    "    values_over_time.append(VP.copy())\n",
    "\n",
    "# Plot the value function over time\n",
    "for i, values in enumerate(values_over_time):\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        plt.plot(values, label=f'Episode {i + 1}')\n",
    "\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Value Function Over Time')\n",
    "plt.show()\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
